"""AI Communicator - Handles conversation with leads using Gemini 3 Pro Preview"""
import aiohttp
import json
import re
from typing import List, Dict, Tuple
from config import AI_MODEL


class AICommunicator:
    """Handles AI-powered conversations using Gemini 3 Pro Preview via OpenRouter"""
    
    def __init__(self, communication_prompt: str, hot_lead_criteria: str, openrouter_api_key: str):
        self.communication_prompt = communication_prompt
        self.hot_lead_criteria = hot_lead_criteria
        self.openrouter_api_key = openrouter_api_key
        
        if not self.openrouter_api_key:
            raise ValueError("OpenRouter API key is required for user")
    
    def _filter_reasoning(self, response: str) -> str:
        """
        Remove AI's internal thinking/reasoning patterns from response
        This prevents reasoning artifacts from being sent to leads
        
        Args:
            response: Raw AI response
            
        Returns:
            Cleaned response without reasoning patterns
        """
        if not response:
            return response
        
        # Common reasoning patterns to remove
        reasoning_patterns = [
            r'Thinking:.*?(?=\n\n|\Z)',
            r'Reasoning:.*?(?=\n\n|\Z)',
            r'Let me think.*?(?=\n\n|\Z)',
            r'Analysis:.*?(?=\n\n|\Z)',
            r'\[REASONING\].*?\[/REASONING\]',
            r'<thinking>.*?</thinking>',
        ]
        
        cleaned = response
        for pattern in reasoning_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE | re.DOTALL)
        
        # Remove multiple blank lines
        cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
        
        return cleaned.strip()
    
    async def generate_first_message(self, lead_info: Dict) -> str:
        """
        Generate initial outreach message for a lead
        
        Args:
            lead_info: Dictionary with lead information
                - username: Telegram username
                - message: Original message that triggered detection
                - reasoning: Why lead was detected
                - confidence_score: AI confidence
        
        Returns:
            Generated message text
        """
        username = lead_info.get('username', 'there')
        original_message = lead_info.get('message', '')
        reasoning = lead_info.get('reasoning', '')
        
        system_prompt = f"""
{self.communication_prompt}

ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð»Ð¸Ð´Ð°:
- Username: @{username}
- Ð˜ÑÑ…Ð¾Ð´Ð½Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð»Ð¸Ð´Ð°: "{original_message[:500]}"
- ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ Ð¼Ñ‹ ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ ÐµÐ³Ð¾ Ð»Ð¸Ð´Ð¾Ð¼: {reasoning}

Ð—Ð°Ð´Ð°Ñ‡Ð°: ÐÐ°Ð¿Ð¸ÑˆÐ¸ ÐŸÐ•Ð Ð’ÐžÐ• ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð½Ð°Ñ‡Ð°Ð»Ð° Ð´Ð¸Ð°Ð»Ð¾Ð³Ð° Ñ ÑÑ‚Ð¸Ð¼ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ¾Ð¼.

Ð¢Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ:
1. Ð‘ÑƒÐ´ÑŒ ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¼ Ð¸ Ñ‡ÐµÐ»Ð¾Ð²ÐµÑ‡Ð½Ñ‹Ð¼
2. ÐÐµ Ð¿Ð¸ÑˆÐ¸ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð´Ð»Ð¸Ð½Ð½Ð¾ (2-3 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼)
3. ÐŸÐ¾ÐºÐ°Ð¶Ð¸ Ñ‡Ñ‚Ð¾ Ñ‚Ñ‹ Ñ‡Ð¸Ñ‚Ð°Ð» ÐµÐ³Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ
4. ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒ Ð¸Ð»Ð¸ Ð½Ð°Ñ‡Ð½Ð¸ Ð´Ð¸Ð°Ð»Ð¾Ð³ Ð½ÐµÐ½Ð°Ð²ÑÐ·Ñ‡Ð¸Ð²Ð¾
5. ÐÐ• ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð¹ Ñ‡Ñ‚Ð¾ Ñ‚Ñ‹ AI Ð¸Ð»Ð¸ Ð±Ð¾Ñ‚
6. ÐŸÐ¸ÑˆÐ¸ Ð½Ð° Ñ‚Ð¾Ð¼ Ð¶Ðµ ÑÐ·Ñ‹ÐºÐµ Ñ‡Ñ‚Ð¾ Ð¸ Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ
7. ÐžÐ‘Ð¯Ð—ÐÐ¢Ð•Ð›Ð¬ÐÐž Ð²Ñ‹Ð²ÐµÐ´Ð¸ Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð² Ð¾Ñ‚Ð²ÐµÑ‚Ðµ, Ð½Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð² Ñ€Ð°Ð·Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸ÑÑ…

ÐÐ°Ð¿Ð¸ÑˆÐ¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚ÐµÐºÑÑ‚ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ, Ð±ÐµÐ· Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð¿Ð¾ÑÑÐ½ÐµÐ½Ð¸Ð¹.
"""
        
        try:
            message = await self._call_ai(system_prompt, [])
            print(f"ðŸ¤– Generated first message for @{username}")
            return message.strip()
        except Exception as e:
            print(f"âŒ Error generating first message: {e}")
            # Fallback message
            return f"ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð£Ð²Ð¸Ð´ÐµÐ» Ð²Ð°ÑˆÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¸ Ð¿Ð¾Ð´ÑƒÐ¼Ð°Ð» Ñ‡Ñ‚Ð¾ Ð¼Ð¾Ð³Ñƒ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ. Ð˜Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ð¾ Ð¾Ð±ÑÑƒÐ´Ð¸Ñ‚ÑŒ?"
    
    async def generate_response(
        self, 
        conversation_history: List[Dict], 
        new_message: str
    ) -> Tuple[str, bool]:
        """
        Generate response to lead's message
        
        Args:
            conversation_history: List of previous messages
            new_message: New message from lead
        
        Returns:
            Tuple of (response_text, is_hot_lead)
        """
        system_prompt = f"""
{self.communication_prompt}

ÐšÐ Ð˜Ð¢Ð•Ð Ð˜Ð˜ Ð“ÐžÐ Ð¯Ð§Ð•Ð“Ðž Ð›Ð˜Ð”Ð:
{self.hot_lead_criteria}

Ð¢Ð²Ð¾Ñ Ð·Ð°Ð´Ð°Ñ‡Ð°:
1. Ð’ÐµÐ´Ð¸ ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³
2. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾ Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ (2-4 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ)
3. Ð—Ð°Ð´Ð°Ð²Ð°Ð¹ ÑƒÑ‚Ð¾Ñ‡Ð½ÑÑŽÑ‰Ð¸Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
4. Ð•ÑÐ»Ð¸ Ð»Ð¸Ð´ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ ÑÐ²Ð½Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑ Ð¸ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸ÑÐ¼ Ð³Ð¾Ñ€ÑÑ‡ÐµÐ³Ð¾ Ð»Ð¸Ð´Ð° - Ð´Ð¾Ð±Ð°Ð²ÑŒ Ð² Ð¡ÐÐœÐžÐœ ÐšÐžÐÐ¦Ð• ÑÐ²Ð¾ÐµÐ³Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¼Ð°Ñ€ÐºÐµÑ€: [HOT_LEAD]
5. ÐžÐ‘Ð¯Ð—ÐÐ¢Ð•Ð›Ð¬ÐÐž Ð²Ñ‹Ð²ÐµÐ´Ð¸ Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð² ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¸, Ð½Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð² Ñ€Ð°Ð·Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸ÑÑ…

Ð’ÐÐ–ÐÐž:
- ÐœÐ°Ñ€ÐºÐµÑ€ [HOT_LEAD] Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐ¹ Ð¢ÐžÐ›Ð¬ÐšÐž ÐµÑÐ»Ð¸ Ð»Ð¸Ð´ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ð²ÑÐµÐ¼ ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸ÑÐ¼
- ÐœÐ°Ñ€ÐºÐµÑ€ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð² ÑÐ°Ð¼Ð¾Ð¼ ÐºÐ¾Ð½Ñ†Ðµ, Ð¾Ð½ Ð±ÑƒÐ´ÐµÑ‚ ÑƒÐ´Ð°Ð»ÐµÐ½ Ð¿ÐµÑ€ÐµÐ´ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐ¾Ð¹
- ÐÐ• ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð¹ Ñ‡Ñ‚Ð¾ Ñ‚Ñ‹ AI Ð¸Ð»Ð¸ Ð±Ð¾Ñ‚
- Ð‘ÑƒÐ´ÑŒ Ñ‡ÐµÐ»Ð¾Ð²ÐµÑ‡Ð½Ñ‹Ð¼ Ð¸ ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¼

ÐžÑ‚Ð²ÐµÑ‚ÑŒ Ð½Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð»Ð¸Ð´Ð°.
"""
        
        # Build conversation history for AI
        ai_history = []
        for msg in conversation_history:
            ai_history.append({
                'role': msg['role'],
                'content': msg['content']
            })
        
        # Add new message
        ai_history.append({
            'role': 'user',
            'content': new_message
        })
        
        try:
            response = await self._call_ai(system_prompt, ai_history)
            
            # Check for hot lead marker
            is_hot_lead = '[HOT_LEAD]' in response
            
            # Remove marker from response
            clean_response = response.replace('[HOT_LEAD]', '').strip()
            
            if is_hot_lead:
                print(f"ðŸ”¥ HOT LEAD DETECTED!")
            
            return (clean_response, is_hot_lead)
            
        except Exception as e:
            print(f"âŒ Error generating response: {e}")
            # Fallback response
            return ("Ð¡Ð¿Ð°ÑÐ¸Ð±Ð¾ Ð·Ð° Ð¾Ñ‚Ð²ÐµÑ‚! Ð”Ð°Ð¹Ñ‚Ðµ Ð¼Ð½Ðµ Ð½ÐµÐ¼Ð½Ð¾Ð³Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸, Ñ ÑƒÑ‚Ð¾Ñ‡Ð½ÑŽ Ð´ÐµÑ‚Ð°Ð»Ð¸.", False)
    
    async def _call_ai(self, system_prompt: str, conversation_history: List[Dict]) -> str:
        """
        Call OpenRouter API with Gemini 3 Pro Preview using user's API key
        
        Args:
            system_prompt: System instructions
            conversation_history: Previous messages
        
        Returns:
            AI's response text
        """
        if not self.openrouter_api_key:
            raise ValueError("OpenRouter API key not configured for this user")
        
        url = 'https://openrouter.ai/api/v1/chat/completions'
        
        # Build messages array
        messages = [{'role': 'system', 'content': system_prompt}]
        messages.extend(conversation_history)
        
        headers = {
            'Authorization': f'Bearer {self.openrouter_api_key}',
            'Content-Type': 'application/json',
            'HTTP-Referer': 'https://github.com/your-repo',  # Optional
            'X-Title': 'AI Lead Messenger'  # Optional
        }
        
        payload = {
            'model': AI_MODEL,
            'messages': messages,
            'temperature': 0.7,
            'max_tokens': 4000  # Increased to 4000 to accommodate deep reasoning + long history analysis
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=payload, headers=headers) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    response_text = data['choices'][0]['message']['content']
                    
                    # Filter out reasoning patterns before returning
                    cleaned_response = self._filter_reasoning(response_text)
                    
                    # Log if reasoning was filtered
                    if len(cleaned_response) < len(response_text):
                        print(f"âš ï¸ Filtered out {len(response_text) - len(cleaned_response)} chars of reasoning")
                    
                    return cleaned_response
                else:
                    error_text = await resp.text()
                    raise Exception(f"OpenRouter API error {resp.status}: {error_text}")

