import { getOpenRouter } from '../config/openrouter.js';
import { estimateTokens, calculateCost } from '../utils/tokenCounter.js';
import logger from '../utils/logger.js';
import { AIServiceError, retryWithBackoff } from '../utils/errorHandler.js';

/**
 * Filter out reasoning/thinking patterns from AI response
 * Gemini models (especially 3 Pro) might include internal reasoning
 * @param {string} response - Raw AI response
 * @returns {string} Cleaned response without reasoning artifacts
 */
const filterReasoning = (response) => {
  if (!response) return response;
  
  // Common reasoning patterns to remove
  const reasoningPatterns = [
    /Thinking:.*?(?=\n\n|\n[A-Z–ê-–Ø]|\Z)/gis,     // "Thinking: ..."
    /Reasoning:.*?(?=\n\n|\n[A-Z–ê-–Ø]|\Z)/gis,    // "Reasoning: ..."
    /Let me think.*?(?=\n\n|\n[A-Z–ê-–Ø]|\Z)/gis,  // "Let me think..."
    /Analysis:.*?(?=\n\n|\n[A-Z–ê-–Ø]|\Z)/gis,     // "Analysis: ..."
    /\[REASONING\].*?\[\/REASONING\]/gis,         // [REASONING]...[/REASONING]
    /<thinking>.*?<\/thinking>/gis,              // <thinking>...</thinking>
  ];
  
  let cleaned = response;
  for (const pattern of reasoningPatterns) {
    cleaned = cleaned.replace(pattern, '');
  }
  
  // Remove multiple blank lines
  cleaned = cleaned.replace(/\n{3,}/g, '\n\n');
  
  return cleaned.trim();
};

/**
 * Generate sales message suggestion for a lead
 * @param {object} lead - Lead message data
 * @param {object} analysis - AI analysis result
 * @param {string} messagePrompt - User's prompt for message generation
 * @param {string} apiKey - OpenRouter API key
 * @returns {object} Generated suggestion
 */
export const generateMessageSuggestion = async (lead, analysis, messagePrompt, apiKey) => {
  const startTime = Date.now();
  
  try {
    // Build prompt for message generation
    const systemPrompt = `–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ sales –º–µ–Ω–µ–¥–∂–µ—Ä–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥—Å–∫–∞–∑–∫—É –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–º—É –∫–ª–∏–µ–Ω—Ç—É (–ª–∏–¥—É).

–í–ê–ñ–ù–û:
- –ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
- –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º
- –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ª–∏–¥–µ
- –°–ª–µ–¥—É–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≥–æ—Ç–æ–≤—ã–º —à–∞–±–ª–æ–Ω–æ–º —Å–æ–æ–±—â–µ–Ω–∏—è
- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –≤—ã–≤–µ–¥–∏ –≥–æ—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –ø–æ–¥—Å–∫–∞–∑–∫–∏ –≤ –æ—Ç–≤–µ—Ç–µ, –Ω–µ —Ç–æ–ª—å–∫–æ –≤ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è—Ö`;

    // Auto-replace placeholders in user prompt
    const processedPrompt = messagePrompt
      .replace(/\[–Ω–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ç–∞\]/gi, lead.chat_name || '—ç—Ç–æ–º —á–∞—Ç–µ')
      .replace(/\[—á–∞—Ç\]/gi, lead.chat_name || '—á–∞—Ç–µ')
      .replace(/\[–∏–º—è\]/gi, lead.first_name || '–¥—Ä—É–≥')
      .replace(/\[username\]/gi, lead.username ? '@' + lead.username : '–≤—ã');
    
    const userPrompt = `–ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:
${processedPrompt}

–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –õ–ò–î–ï:
–ò–º—è: ${lead.first_name || ''} ${lead.last_name || ''}
Username: ${lead.username ? '@' + lead.username : '–°–∫—Ä—ã—Ç'}
–ë–∏–æ: ${lead.bio || '–ù–µ —É–∫–∞–∑–∞–Ω–æ'}
–ö–∞–Ω–∞–ª: ${lead.chat_name || '–ù–µ —É–∫–∞–∑–∞–Ω'}

–°–û–û–ë–©–ï–ù–ò–ï –õ–ò–î–ê:
${lead.message}

–ê–ù–ê–õ–ò–ó AI:
–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: ${analysis.confidence_score}%
–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: ${analysis.reasoning}
–°–æ–≤–ø–∞–¥–µ–Ω–∏—è: ${analysis.matched_criteria?.join(', ') || '–ù–µ—Ç'}

–ó–ê–î–ê–ß–ê: –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫—É—é –ø–æ–¥—Å–∫–∞–∑–∫—É (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –¥–ª—è sales –º–µ–Ω–µ–¥–∂–µ—Ä–∞ - –∫–∞–∫–æ–µ –ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∞–ø–∏—Å–∞—Ç—å —ç—Ç–æ–º—É –ª–∏–¥—É. –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤—ã—à–µ.`;

    // Estimate cost
    const inputTokens = estimateTokens(systemPrompt) + estimateTokens(userPrompt);
    const estimatedOutputTokens = 150; // Short suggestion

    logger.debug('Generating message suggestion', {
      leadId: lead.id,
      estimatedInputTokens: inputTokens,
      estimatedOutputTokens
    });

    // Get OpenRouter client
    const client = getOpenRouter(apiKey);
    // Use Gemini 3 Pro for high-quality suggestions
    // With increased max_tokens (2500) to allow room for reasoning + content and reasoning filter, it works reliably
    const model = process.env.MESSAGE_SUGGESTION_MODEL || 'google/gemini-3-pro-preview';

    logger.info('üöÄ Starting OpenRouter API call for suggestion', {
      leadId: lead.id,
      model,
      systemPromptLength: systemPrompt.length,
      userPromptLength: userPrompt.length,
      temperature: 0.7,
      max_tokens: 2500
    });

    // Make API call
    let response;
    try {
      response = await retryWithBackoff(async () => {
        logger.info('‚è≥ Calling OpenRouter API...', { leadId: lead.id, model });
        const apiResponse = await client.chat.completions.create({
          model,
          messages: [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: userPrompt }
          ],
          temperature: 0.7, // More creative for message generation
          max_tokens: 2500 // Further increased - Gemini 3 Pro uses lots of reasoning tokens
          // NOTE: No provider filtering for Gemini models - they're only available via Google
        });
        logger.info('‚úÖ OpenRouter API call completed', { 
          leadId: lead.id, 
          hasResponse: !!apiResponse,
          responseKeys: apiResponse ? Object.keys(apiResponse) : []
        });
        return apiResponse;
      }, 3, 1000);
    } catch (apiError) {
      logger.error('‚ùå OpenRouter API call failed', {
        leadId: lead.id,
        error: apiError.message,
        stack: apiError.stack,
        name: apiError.name
      });
      throw apiError;
    }

    const duration = Date.now() - startTime;

    // Log full response for debugging
    logger.info('OpenRouter API response received', {
      leadId: lead.id,
      hasChoices: !!response.choices,
      choicesLength: response.choices?.length,
      hasMessage: !!response.choices?.[0]?.message,
      hasContent: !!response.choices?.[0]?.message?.content,
      contentLength: response.choices?.[0]?.message?.content?.length,
      contentPreview: response.choices?.[0]?.message?.content?.substring(0, 100),
      model: response.model,
      usage: response.usage
    });

    // Extract response
    const rawSuggestion = response.choices[0]?.message?.content;
    if (!rawSuggestion) {
      logger.error('Empty response from AI - full response', {
        leadId: lead.id,
        response: JSON.stringify(response, null, 2)
      });
      throw new AIServiceError('Empty response from AI');
    }

    // Filter out reasoning/thinking patterns
    const suggestion = filterReasoning(rawSuggestion);
    
    // Log if reasoning was filtered out
    if (suggestion.length < rawSuggestion.length) {
      logger.info('Filtered reasoning from suggestion', {
        leadId: lead.id,
        originalLength: rawSuggestion.length,
        cleanedLength: suggestion.length,
        filtered: rawSuggestion.length - suggestion.length
      });
    }

    // Calculate actual cost
    const actualInputTokens = response.usage?.prompt_tokens || inputTokens;
    const actualOutputTokens = response.usage?.completion_tokens || estimatedOutputTokens;
    const cost = calculateCost(actualInputTokens, actualOutputTokens);

    logger.info('Message suggestion generated', {
      leadId: lead.id,
      duration,
      cost: cost.totalCost
    });

    return {
      success: true,
      suggestion: suggestion.trim(),
      metadata: {
        model,
        duration,
        cost: cost.totalCost,
        tokens: {
          input: actualInputTokens,
          output: actualOutputTokens,
          total: actualInputTokens + actualOutputTokens
        }
      }
    };

  } catch (error) {
    const duration = Date.now() - startTime;

    logger.error('Failed to generate message suggestion', {
      leadId: lead.id,
      error: error.message,
      duration
    });

    throw new AIServiceError('Failed to generate message suggestion', {
      leadId: lead.id,
      originalError: error.message,
      duration
    });
  }
};

export default {
  generateMessageSuggestion
};

